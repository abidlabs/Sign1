{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Alphabet Classifier Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The goal of this Sign2 variation is to experiment with using Mixup Regularization\n",
    "(See \"mixup: Beyond Empirical Risk Minimization\" - Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Pazj, 2018;  https://arxiv.org/abs/1710.09412) \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Datasource: \n",
    "<a href=\"https://www.kaggle.com/grassknoted/asl-alphabet\">https://www.kaggle.com/grassknoted/asl-alphabet</a>\n",
    "\n",
    "<br>\n",
    "\n",
    "This is a prototype, playing with FastAI using Resnet34 to classify American Sign Language alphabet.  It's basically Notebook #2 from the MOOC on a \"clean\" dataset.  We get great results ... but the data is really contrived.  It's highly likely the model will overfit, however it's a good test of the library.\n",
    "<br>\n",
    "<br>\n",
    "### Data Wrangling info\n",
    "The data has been reorganized to put sign images in labelled directories, making it easy to import and sort.\n",
    "We use 2 main data directories and concatenate them together.  the 2nd dataset consists of personally captured images, created using the same notebook that does inference.  We capture every frame of a video and automatically place them in the chosen directory.  The notebook will create the main directory and the label-directory if they don't exist.\n",
    "\n",
    "<img src=\"../docs/images/2021-01-13_00-57.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<HR/>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Notes\n",
    "\n",
    "### SUPER PREPROCESSED\n",
    "\n",
    "* data: 800 external , 300 frank, 300 crazy processed\n",
    "* augmentation True\n",
    "* ~~Label Smoothing~~  Cross-Entropy Loss\n",
    "* pretrained = false\n",
    "* bilinear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from fastbook import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import *\n",
    "import fastai\n",
    "\n",
    "\n",
    "USES_TIMM = True\n",
    "USES_GCHKPT = True\n",
    "ARCH = 'densenetblur121d'   # xresnet50 # resnet101\n",
    "\n",
    "CHOSEN_SAMPLE_SIZE = 2500  # use this to control per-category sample sizes # 1000\n",
    "TEST_SET_SIZE = round(CHOSEN_SAMPLE_SIZE * 0.2)  # number of images per category to put in the test set\n",
    "\n",
    "remove_from_sample = {\n",
    "#         'A': 0.5,\n",
    "#     'B': 0.2,\n",
    "#     'C': 0.2,\n",
    "# #     'D': 0.3,\n",
    "#     'E': 0.2,\n",
    "#     'F': 0.2,\n",
    "#     'H': 0.3,\n",
    "# #     'L': 0.7,\n",
    "#     'M': 0.3,\n",
    "# #     'N': 0.9,\n",
    "# #     'O': 0.2,\n",
    "# #     'Q': 0.75,\n",
    "# #     'R': 0.4,\n",
    "#     'U': 0.2\n",
    "# #     'Y': 0.8 \n",
    "\n",
    "}  # fractions of original samples to keep\n",
    "\n",
    "add_to_sample = {  # Use this minimally until we add NEW samples rather than oversampling.\n",
    "#     'A': 0.2,\n",
    "#     'B': 0.2,\n",
    "#     'G': 0.1,\n",
    "#     'I': 0.3,\n",
    "#     'K': 0.2,\n",
    "#     'L': 0.2,\n",
    "#     'N': 0.3,\n",
    "#     'O': 0.1,\n",
    "#     'R': 0.3,\n",
    "#     'S': 0.4,\n",
    "#     'T': 0.4,\n",
    "#     'V': 0.1,\n",
    "#     'W': 0.1,\n",
    "#     'Y': 0.2\n",
    "}\n",
    "\n",
    "FROZEN_EPOCHS = 1  # 1\n",
    "EPOCHS = 40  # 4\n",
    "BATCH_SIZE = 28  # 16\n",
    "RESOLUTION = 280  # 300\n",
    "\n",
    "PRETRAINED_FLAG = True\n",
    "\n",
    "data = 'combined3-proc2'\n",
    "rn_addon = f'_data={data}'\n",
    "time = datetime.today().strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "RUN_NAME = f'{time} - arch={ARCH.__name__} - samples={CHOSEN_SAMPLE_SIZE} frozen={FROZEN_EPOCHS} epochs={EPOCHS} bs={BATCH_SIZE} res={RESOLUTION} {rn_addon}'\n",
    "print(f\"RUN_NAME = '{RUN_NAME}'\")\n",
    "\n",
    "\n",
    "# set this to None if training a new model from scratch\n",
    "PREV_TRAINED_MODEL_RUN_NAME = None\n",
    "PREV_TRAINED_MODEL_EPOCH = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# %matplotlib inline\n",
    "%matplotlib widget\n",
    "plt.rcParams['figure.figsize'] = [9, 5]\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "%env WANDB_WATCH=false\n",
    "\n",
    "import wandb\n",
    "from fastai.callback.wandb import *\n",
    "\n",
    "wandb.init(project=\"asl-sign-language-recognition\", mode='disabled')\n",
    "wandb.run.name = RUN_NAME\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '../data/external/Training_Set'\n",
    "path = f'../data/{data}/Training_Set'\n",
    "# path2 = '../data/frank-ledlights-L'\n",
    "path2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for an available GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('CUDA available: '.ljust(28), torch.cuda.is_available())\n",
    "print('CUDA device count: '.ljust(28), torch.cuda.device_count())\n",
    "\n",
    "current_device = torch.cuda.current_device()\n",
    "print('Current CUDA Device index: '.ljust(28), current_device)\n",
    "# torch.cuda.device(current_device)\n",
    "print('Current CUDA Device: '.ljust(28), torch.cuda.get_device_name(current_device))\n",
    "print()\n",
    "# print('CUDA available: '.ljust(24), torch.cuda.is_available())\n",
    "print(f'fastai version:              {fastai.__version__}')\n",
    "# print(f'fastcore version:            {fastcore.__version__}')\n",
    "# print(f'fastbook version:            {fastbook.__version__}')\n",
    "print(f'cuda version:                {torch.version.cuda}')\n",
    "print(f'torch version:               {torch.__version__}')\n",
    "# print(f'python version:              {python_version()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Dataset Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_uppercase\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_files = {}\n",
    "image_files_qty = {}\n",
    "\n",
    "# loop through all the characters to build dictionaries of image files and quartity of each category\n",
    "for c, i in zip(ascii_uppercase, np.arange(len(ascii_uppercase))):\n",
    "    \n",
    "    if c == 'J' or c == 'Z': continue  # we don't use these characters\n",
    "        \n",
    "    image_files[c] = get_image_files(path + f'/{c}')\n",
    "    if path2 != None:\n",
    "        image_files[c] += get_image_files(path2 + f'/{c}')\n",
    "\n",
    "        \n",
    "    l = len(image_files[c])\n",
    "    image_files_qty[c] = l\n",
    "    \n",
    "# custom code since we don't use 'Z'\n",
    "# image_files.pop('J')\n",
    "# image_files_qty.pop('J')\n",
    "# image_files.pop('Z')\n",
    "# image_files_qty.pop('Z')\n",
    "\n",
    "# Get the character with the largest and smallest number of entries\n",
    "maxqc = max(image_files_qty, key=image_files_qty.get)\n",
    "minqc = min(image_files_qty, key=image_files_qty.get)\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Character with the most images:   {maxqc},   with {image_files_qty[maxqc]} images')\n",
    "print(f'Character with the least images:  {minqc},   with {image_files_qty[minqc]} images')\n",
    "print(f'Average number of images:         {round(np.mean(list(image_files_qty.values())))}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the number of image files for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(image_files_qty.items)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(image_files_qty.keys(), image_files_qty.values())\n",
    "plt.axhline(CHOSEN_SAMPLE_SIZE, ls='--', color='r', label='Per-Category Sample Size')\n",
    "plt.xlabel('Hand Sign Categories')\n",
    "plt.ylabel('Sample Size')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the Dataset with Oversampling or Undersampling\n",
    "\n",
    "Sampled size is modulated by variable `CHOSEN_SAMPLE_SIZE` and will oversample or undersample (or both) depending on the amount of data available vs the chosen sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "maxq = image_files_qty[maxqc] # the quantity of the largest category\n",
    "minq = image_files_qty[minqc] # the quantity of the smallest category\n",
    "\n",
    "# all_image_files = pd.DataFrame(columns=[0])  # holds all the image files in one dataframe\n",
    "train_image_files = {}\n",
    "test_image_files = pd.DataFrame()\n",
    "\n",
    "for char, q in iter(image_files_qty.items()):\n",
    "    df = pd.DataFrame(data=list(image_files[char]), columns=[0])  # create a dataframe from each list\n",
    "    \n",
    "\n",
    "    # undersample or over sample as needed\n",
    "    if len(df) >= CHOSEN_SAMPLE_SIZE:\n",
    "        df = df.sample(CHOSEN_SAMPLE_SIZE, replace=False)  # undersample\n",
    "    else:\n",
    "        delta = CHOSEN_SAMPLE_SIZE - len(df)\n",
    "        df = pd.concat([df, df.sample(delta, replace=(delta > len(df)))], ignore_index=True)  # oversample\n",
    "        \n",
    "    # siphon off the test set\n",
    "    _tif = df[0].sample(TEST_SET_SIZE, replace=False)\n",
    "    test_image_files[char] = _tif.reset_index(drop=True)\n",
    "    \n",
    "    # form  the training set\n",
    "    train_image_files[char] = df[0].drop(_tif.index)\n",
    "       \n",
    "test_image_files = test_image_files.sample(frac=1)  # shuffle - training set is shuffled later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure the Dataset is Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(train_image_files.keys(), [len(l) for l in train_image_files.values()])\n",
    "# plt.ylim(0, CHOSEN_SAMPLE_SIZE)\n",
    "plt.axhline(CHOSEN_SAMPLE_SIZE, ls='--', color='r', label='Original Sample Size')\n",
    "plt.xlabel('Hand Sign Categories')\n",
    "plt.ylabel('Sample Size of Training Set')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the  Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "ax.bar(test_image_files.count().index, test_image_files.count() / CHOSEN_SAMPLE_SIZE * 100)\n",
    "ax.axhline(100, ls='--', color='r', label='Original Sample')\n",
    "plt.xlabel('Hand Sign Categories')\n",
    "plt.ylabel('Test Sample (%)')\n",
    "formatter = ticker.PercentFormatter()\n",
    "ax.yaxis.set_major_formatter(formatter)\n",
    "plt.legend(loc=(0.5,0.75));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliberately Adjust the Sample Size of Certain Categories to Fine Tune the Model\n",
    "Because the data is so easy to overfit and because we see certain categories fitting faster/stronger than others - causing the model to always select those categories... we try to balance that effect by decreasing the number of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in remove_from_sample:\n",
    "    train_image_files[key] = train_image_files[key].sample(frac=(1 - remove_from_sample[key]))\n",
    "    \n",
    "## @TODO: sample outside the training set rather than oversampling.\n",
    "for key in add_to_sample:\n",
    "    train_image_files[key] = pd.concat([train_image_files[key],pd.Series(train_image_files[key]).sample(frac=add_to_sample[key])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the Training Dataset has been Appropriately Altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_from_sample != {}:\n",
    "    fig, ax = plt.subplots(figsize=(6,3))\n",
    "    plt.bar(train_image_files.keys(), [len(l) for l in train_image_files.values()])\n",
    "    plt.xlabel('Hand Sign Categories')\n",
    "    plt.ylabel('Sample Size')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print('\\nNo dataset alterations were selected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr/>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Model Creation and Training\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Create the DataBlock, while Resizing and Augmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Needed to pass into the DataBlock    \n",
    "def get_fnames(path): \n",
    "    retlist = []\n",
    "\n",
    "    for arr in train_image_files.values():\n",
    "        for f in arr:\n",
    "            retlist.append(f)\n",
    "        \n",
    "    return random.sample(retlist, len(retlist))\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "signs = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock), \n",
    "    get_items=get_fnames, \n",
    "    splitter=RandomSplitter(valid_pct=0.25, seed=42),\n",
    "    get_y=parent_label,\n",
    "    item_tfms=Resize(RESOLUTION, method='bilinear') #,\n",
    "\n",
    "#     item_tfms=CropPad(RESOLUTION, pad_mode='zeros')\n",
    "  ,    batch_tfms=aug_transforms(do_flip=True, size=RESOLUTION, batch=False, max_zoom=1.2, mult=1.5, pad_mode='zeros'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Load the Data by Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = signs.dataloaders(path, bs=BATCH_SIZE)\n",
    "# wandb.log({'dataset':'../data/external/Training Set'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Verify the Training and Validation Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.train.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T12:47:39.994667Z",
     "iopub.status.busy": "2020-09-26T12:47:39.994388Z",
     "iopub.status.idle": "2020-09-26T12:47:39.999492Z",
     "shell.execute_reply": "2020-09-26T12:47:39.998668Z",
     "shell.execute_reply.started": "2020-09-26T12:47:39.994592Z"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Create Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eary stopping callback\n",
    "early_stop_cb = EarlyStoppingCallback(monitor='error_rate', min_delta=0.0001, patience=4)\n",
    "\n",
    "# Save the current model's weights every epoch\n",
    "save_cb = SaveModelCallback(fname=RUN_NAME, every_epoch=True, with_opt=True)\n",
    "\n",
    "# Wandb Callback for logging\n",
    "wandb_cb = WandbCallback(log='all', log_dataset=False)  #, log_dataset=True)\n",
    "\n",
    "# Mixup callback for regularization\n",
    "# mixup_cb = MixUp(alpha=0.2)\n",
    "mixup_cb = None\n",
    "\n",
    "# Cutmix callback for regularization\n",
    "#cutmix_cb = CutMix(alpha=0.4)\n",
    "cutmix_cb = None\n",
    "\n",
    "\n",
    "# List of callbacks to be used later\n",
    "cbs = [ShowGraphCallback(), save_cb, early_stop_cb]\n",
    "\n",
    "if mixup_cb != None:\n",
    "    cbs.insert(0, mixup_cb)\n",
    "    \n",
    "if cutmix_cb != None:\n",
    "    cbs.insert(0, cutmix_cb)\n",
    "\n",
    "print('\\nAll Callbacks: ', cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the effect of CutMix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cutmix_cb != None:\n",
    "    with Learner(dls, nn.Linear(3,4), loss_func=CrossEntropyLossFlat(), cbs=cutmix_cb) as learn:\n",
    "        learn.epoch,learn.training = 0,True\n",
    "        learn.dl = dls.train\n",
    "        b = dls.one_batch()\n",
    "        learn._split(b)\n",
    "        learn('before_batch')\n",
    "\n",
    "    _,axs = plt.subplots(3,3, figsize=(9,9))\n",
    "    dls.show_batch(b=(cutmix_cb.x,cutmix_cb.y), ctxs=axs.flatten())\n",
    "    \n",
    "else: print('\\n CutMix was not selected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Visualize the Effect of MixUp\n",
    "\n",
    "MixUp creates a linear interpolation between the target data and another datapoint.  In images, it shows up as ghostly figures.  The technique has been shown to be a good to decrease the liklihood of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mixup_cb != None:\n",
    "    with Learner(dls, nn.Linear(3,4), loss_func=CrossEntropyLossFlat(), cbs=mixup_cb) as learn:\n",
    "        learn.epoch,learn.training = 0,True\n",
    "        learn.dl = dls.train\n",
    "        b = dls.one_batch()\n",
    "        learn._split(b)\n",
    "        learn('before_batch')\n",
    "\n",
    "    _,axs = plt.subplots(3,3, figsize=(9,9))\n",
    "    dls.show_batch(b=(mixup_cb.x,mixup_cb.y), ctxs=axs.flatten())\n",
    "\n",
    "else: print('\\n MixUp was not selected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.log()\n",
    "\n",
    "# learn = cnn_learner(dls, arch=ARCH, metrics=[error_rate, accuracy], cbs=cbs, loss_func=LabelSmoothingCrossEntropy(), pretrained=PRETRAINED_FLAG)\n",
    "learn = cnn_learner(dls, arch=ARCH, metrics=[error_rate, accuracy], cbs=cbs, pretrained=PRETRAINED_FLAG)\n",
    "\n",
    "if PREV_TRAINED_MODEL_RUN_NAME != None:\n",
    "#     learn = load_learner(f'../models/{PREV_TRAINED_MODEL_RUN_NAME}.pkl', cpu=False)\n",
    "    load_model(f'models/{PREV_TRAINED_MODEL_RUN_NAME}_{PREV_TRAINED_MODEL_EPOCH}.pth', learn, opt=Adam, with_opt=False)\n",
    "    print(f'Loaded model from: {PREV_TRAINED_MODEL_RUN_NAME} @EPOCH #{PREV_TRAINED_MODEL_EPOCH}')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the Loss Function, Optimization Function and Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLoss Function: ', learn.loss_func)\n",
    "print('\\nOptimization Function: ', learn.opt_func)\n",
    "# print('\\n\\n', learn.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Find a Good Learning Rate to Start With"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_DIV = 14e0  # Shift the lr_min left by this amount.  Adjust as necessary\n",
    "lr_min = 0.002  # just a default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRETRAINED_FLAG == True:\n",
    "    learn.freeze()\n",
    "    lr_min,lr_steep = learn.lr_find()\n",
    "    plt.axvline(lr_min, ls='--', color='red', label=f'lr_min={round(lr_min,6)}')\n",
    "    plt.axvline(lr_min/LR_DIV, ls='--', color='yellow', label=f'lr_min / {LR_DIV}={round(lr_min/LR_DIV,6)}')\n",
    "    plt.axvline(lr_steep, ls='--', color='grey', label=f'lr_steep={round(lr_steep,6)}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}, (Mim/10)/{LR_DIV}: {lr_min/LR_DIV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick a Good Initial Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR_CHOICE = lr_min/LR_DIV\n",
    "# LR_CHOICE = lr_steep\n",
    "LR_CHOICE = 0.0004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "## Fit the last layers, unfreeze, fit the whole net, with a decent initial LR, all in one go.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f'FROZEN_EPOCHS:  {FROZEN_EPOCHS}')\n",
    "print(f'EPOCHS:         {EPOCHS}')\n",
    "print(f'Learning Rate:  {LR_CHOICE}\\n\\n')\n",
    "\n",
    "# learn.fine_tune(EPOCHS, freeze_epochs=FROZEN_EPOCHS, base_lr=lr_min/LR_DIV)\n",
    "#learn.fine_tune(EPOCHS, freeze_epochs=0)\n",
    "if PRETRAINED_FLAG == True:\n",
    "    learn.fit_one_cycle(FROZEN_EPOCHS, LR_CHOICE)\n",
    "\n",
    "# learn.fine_tune(1, base_lr=lr_min/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually set up the unfrozen runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_min,lr_steep = learn.lr_find()\n",
    "plt.axvline(lr_min, ls='--', color='red', label=f'lr_min={round(lr_min,6)}')\n",
    "plt.axvline(LR_CHOICE/2, ls='--', color='yellow', label=f'LR_CHOICE/2={round(LR_CHOICE/2,6)}')\n",
    "plt.axvline(lr_steep, ls='--', color='grey', label=f'lr_steep={round(lr_steep,6)}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}, LR_CHOICE/2: {LR_CHOICE / 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a new Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR_CHOICE_UNFROZ = lr_min/LR_DIV\n",
    "# LR_CHOICE_UNFROZ = LR_CHOICE / 2\n",
    "LR_CHOICE_UNFROZ = 0.00015\n",
    "# LR_CHOICE_UNFROZ = 10e-5\n",
    "# LR_CHOICE_UNFROZ = 0.00008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'EPOCHS:         {EPOCHS}')\n",
    "print(f'Learning Rate:  {LR_CHOICE_UNFROZ}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(EPOCHS, LR_CHOICE_UNFROZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fine_tune(1, freeze_epochs=1, base_lr=5e-5, cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fit(1, cbs=[mixup_cb, ShowGraphCallback(), early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## Persist the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(f'../models/{RUN_NAME}.pkl')\n",
    "# path = Path('../models')\n",
    "# path.ls(file_exts='.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Set (not test set) Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.recorder.plot_loss()\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Batches Processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Visualize with a confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.most_confused()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.print_classification_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Show the top 15 most error prone images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(15, nrows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### ~~Clean the images that were hard to interpret and obviously bad~~\n",
    "\n",
    "(I had to remove this section as it was using too much RAM and crashing the kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a Widget that allows us to mark poor exemplars for deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# cleaner = None\n",
    "# gc.collect()\n",
    "\n",
    "# cleaner = ImageClassifierCleaner(learn)\n",
    "# cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the Indexes of the images we want to delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaner.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# for idx in cleaner.delete():\n",
    "#     print(f'removing: {str(cleaner.fns[idx])}')\n",
    "#     os.remove(str(cleaner.fns[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set Analysis\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gc\n",
    "cleaner = None\n",
    "test_learn = None\n",
    "inputs = None\n",
    "preds = None\n",
    "interp = None\n",
    "targs = None\n",
    "decoded = None\n",
    "losses = None\n",
    "# learn = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_learn = learn\n",
    "num = 3\n",
    "# RUN_NAME='20210122-0037 - arch=xresnet50_deep - samples=3000 frozen=1 epochs=40 bs=16 res=360 _data=external'\n",
    "# RUN_NAME = '20210122-2356 - arch=xresnet50_deep - samples=3000 frozen=1 epochs=40 bs=16 res=360 _data=external'\n",
    "\n",
    "# test_learn = load_learner(f'../models/{RUN_NAME}.pkl', cpu=False)\n",
    "# load_model(f'models/{RUN_NAME}_{num}.pth', test_learn, opt=Adam, with_opt=False)\n",
    "\n",
    "\n",
    "\n",
    "test_learn = learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_fnames(path):\n",
    "    return random.sample(list(test_image_files.values.flatten()), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock), \n",
    "    get_items=get_test_fnames,\n",
    "    get_y=parent_label, \n",
    "    item_tfms=Resize(RESOLUTION, method='bilinear')) #,\n",
    "#     item_tfms=CropPad(RESOLUTION, pad_mode='zeros'))\n",
    "#    ,    batch_tfms=aug_transforms(do_flip=True, size=RESOLUTION, batch=False, max_zoom=1.0, mult=1, pad_mode='zeros'))\n",
    "\n",
    "\n",
    "dls = test_db.dataloaders(path, bs=BATCH_SIZE)\n",
    "\n",
    "test_dl = dls.test_dl(get_test_fnames('None'), with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_learn = load_learner(f'../models/{RUN_NAME}.pkl', cpu=False)\n",
    "# test_learn = cnn_learner(dls, arch=ARCH, metrics=[error_rate, accuracy], cbs=cbs, loss_func=LabelSmoothingCrossEntropy(), pretrained=PRETRAINED_FLAG)\n",
    "# RUN_NAME = '20210119-0130 - arch=xresnet50 - samples=1000 frozen=0 epochs=25 bs=20 res=300 _data=external'\n",
    "# load_model(f'models/{RUN_NAME}_1.pth', test_learn, opt=Adam)\n",
    "\n",
    "\n",
    "# test_learn = cnn_learner(dls, arch=ARCH, metrics=[error_rate, accuracy], pretrained=PRETRAINED_FLAG)\n",
    "# test_learn.load(f'{RUN_NAME}_24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Get the Inferrences on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, preds, targs, decoded, losses = test_learn.get_preds(dl=test_dl, with_input=True, with_decoded=True, with_loss=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation(dl=test_dl, inputs=inputs, preds=preds, targs=targs, decoded=decoded, losses=losses )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_learn.get_preds()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-19T11:34:38.275317Z",
     "iopub.status.busy": "2021-01-19T11:34:38.275201Z",
     "iopub.status.idle": "2021-01-19T11:34:38.281831Z",
     "shell.execute_reply": "2021-01-19T11:34:38.281519Z",
     "shell.execute_reply.started": "2021-01-19T11:34:38.275306Z"
    }
   },
   "source": [
    "# pos = 0\n",
    "# neg = 0\n",
    "\n",
    "\n",
    "# for pred, targ in preds, targs:\n",
    "#     if pred == targ: TP = TP + 1\n",
    "#     if pred != targ:\n",
    "            \n",
    "# print(f'Correct: {correct}')\n",
    "# print(f'Wrong:   {wrong}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Visualize with a confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.most_confused()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.print_classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Clean up\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "# Log\n",
    "\n",
    "* started to plot the learning rates and started to use that information while fitting.\n",
    "* Downgraded to resnet34 @ 300px in order to increase the resolution fed to the model from 128px to 300px - this made a major difference.\n",
    "* Decreased the number of training epochs to 6 after experimenting to find the sweet spot. - also positive change\n",
    "* Changed to exclusively use fine-tune() with it's built in freeze_epochs parameter\n",
    "* Changed the Batch Size in order to bring the arch back to resnet101 @300px\n",
    "* Cleaned up the markdown, removed cells and reordered the rest.\n",
    "* Added an Early Stop.  Starting with 0.01 delta.  \n",
    "* Now moving to 0.1 delta\n",
    "* Integrated wandb to keep track of experiments.\n",
    "* added section to balance the dataset through oversampling.\n",
    "\n",
    "** Attempting a batch size of 12 and 384px with resnet101.  long training times.\n",
    "** Also increased the epochs to 4 on the final layer and 7 on the rest.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fine_tune(epochs=2, freeze_epochs=1, base_lr=high)\n",
    "#                 cbs=[mixup_cb, ShowGraphCallback(), SaveModelCallback(), early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low, high = learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive this version of the notehook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Allow Jupyter the opportunity to autosave\n",
    "!sleep 20\n",
    "# time = '20210122-2356'\n",
    "# copy the notebook file - the prefix links it to the saved model\n",
    "shutil.copyfile('Sign3.ipynb', f'.Archive/{time} - Sign3.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # learn = cnn_learner(dls, arch=ARCH, metrics=[error_rate, accuracy], cbs=cbs, loss_func=LabelSmoothingCrossEntropy(), pretrained=PRETRAINED_FLAG)\n",
    "# learn = load_learner('../models/20210118-1443 - arch=resnet50 - samples=480 frozen=0 epochs=6 bs=20 res=300 _data=external.pkl', cpu=False)\n",
    "# learn.dls = dls\n",
    "\n",
    "# lr_min,lr_steep = learn.lr_find()\n",
    "# print(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")\n",
    "\n",
    "# learn.fine_tune(1, freeze_epochs=2, base_lr=lr_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # image = PILImage.create('../data/20210121_132256.jpg')\n",
    "# letter = 'F'\n",
    "# images = [PILImage.create(f'../data/frank/Training_Set/{letter}/1000.jpg'),    PILImage.create(f'../data/frank/Training_Set/{letter}/1008.jpg'),    PILImage.create(f'../data/frank/Training_Set/{letter}/1016.jpg'),    PILImage.create(f'../data/frank/Training_Set/{letter}/1024.jpg'),    PILImage.create(f'../data/frank/Training_Set/{letter}/1032.jpg'),    PILImage.create(f'../data/frank/Training_Set/{letter}/1040.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img in images:\n",
    "#     print(test_learn.predict(img)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_learn.predict(PILImage.create(f'../data/frank/Training_Set/E/1000.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"RUN_NAME = '{RUN_NAME}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
